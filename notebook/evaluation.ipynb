{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d-llm/vinallama-2.7b-chat-orpo\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"d-llm/vinallama-2.7b-chat-orpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24328/3940566827.py:8: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, PatchDPOTrainer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-22 08:57:52.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mStart the dpo training process...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "/tmp/ipykernel_24328/3940566827.py:130: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DPOTrainer.__init__() got an unexpected keyword argument 'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# parser = argparse.ArgumentParser(description=\"Load model with RLHF config:\")\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# parser.add_argument(\"--config\", type=str, default=\"config.yaml\", help=\"Path to config file\")\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# args = parser.parse_args()\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart the dpo training process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43mdpo_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/LLM-Safety-Evaluation/configs/sailor_dpo_config.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m, in \u001b[0;36mdpo_pipeline\u001b[0;34m(config_file_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[1;32m     95\u001b[0m     model,\n\u001b[1;32m     96\u001b[0m     r \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# dpo_trainer = DPOTrainer(\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#     model = model,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#     ref_model = None,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#     max_prompt_length = config['dpo']['max_prompt_length'],\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m dpo_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDPOConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use this for WandB etc\u001b[39;49;00m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_dataset = raw_datasets[\"test\"],\u001b[39;49;00m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m dpo_trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DPOTrainer.__init__() got an unexpected keyword argument 'beta'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "PatchDPOTrainer()\n",
    "\n",
    "from src.processing.data import get_datasets, apply_chat_template\n",
    "from src.utils import load_yaml_config\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "def dpo_pipeline(config_file_path: str):\n",
    "    load_dotenv()\n",
    "    config = load_yaml_config(config_file_path)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Optional set GPU device ID\n",
    "    HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "    login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "\n",
    "    # For full-finetuning - set full_finetuning = True  and 8-bit finetuning - set load_in_8bit = True \n",
    "    if config['model']['load_in_4bit']:\n",
    "        if config['model']['load_in_8bit'] and config['model']['full_finetuning']:\n",
    "            raise Exception(\n",
    "                \"Invalid configuration: You cannot enable both 8-bit loading and full finetuning when 4-bit loading is enabled. \"\n",
    "                \"Choose only one method: set 'full_finetuning = True' for full finetuning, or 'load_in_8bit = True' for 8-bit finetuning.\"\n",
    "            )\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = config['model']['name'],\n",
    "            max_seq_length = config['model']['max_seq_length'],\n",
    "            dtype = config['model']['dtype'],\n",
    "            load_in_4bit = config['model']['load_in_4bit'],\n",
    "            token = HUGGINGFACE_TOKEN\n",
    "        )\n",
    "    elif config['model']['load_in_8bit']:\n",
    "        if config['model']['load_in_4bit'] and config['model']['full_finetuning']:\n",
    "            raise Exception(\n",
    "                \"Invalid configuration: You cannot enable both 4-bit loading and full finetuning when 8-bit loading is enabled. \"\n",
    "                \"Choose only one method: set 'full_finetuning = True' for full finetuning, or 'load_in_4bit = True' for 4-bit finetuning.\"\n",
    "            )\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = config['model']['name'],\n",
    "            max_seq_length = config['model']['max_seq_length'],\n",
    "            dtype = config['model']['dtype'],\n",
    "            load_in_8bit = config['model']['load_in_8bit'],\n",
    "            token = HUGGINGFACE_TOKEN\n",
    "        )\n",
    "    elif config['model']['full_finetuning']:\n",
    "        if config['model']['load_in_8bit'] and config['model']['load_in_4bit']:\n",
    "            raise Exception(\n",
    "                \"Invalid configuration: You cannot enable both 8-bit loading and 4-bit when full finetuning loading is enabled. \"\n",
    "                \"Choose only one method: set 'load_in_4bit = True' for 4-bit finetuning, or 'load_in_8bit = True' for 8-bit finetuning.\"\n",
    "            )\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = config['model']['name'],\n",
    "            max_seq_length = config['model']['max_seq_length'],\n",
    "            dtype = config['model']['dtype'],\n",
    "            full_finetuning = config['model']['load_in_8bit'],\n",
    "            token = HUGGINGFACE_TOKEN\n",
    "        )\n",
    "\n",
    "\n",
    "    raw_datasets = get_datasets(\n",
    "        config['datasets']['sources'], \n",
    "        splits = config['datasets']['splits'],\n",
    "    )\n",
    "    column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs = {\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n",
    "        num_proc = config['datasets']['preprocessing']['num_proc'],\n",
    "        remove_columns = column_names,\n",
    "        desc = \"Formatting comparisons with prompt template\",\n",
    "    )\n",
    "\n",
    "    # # Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "    for split in config['datasets']['splits']:\n",
    "        raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "            {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "        )\n",
    "\n",
    "\n",
    "    # Do model patching and add fast LoRA weights\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = config['lora']['r'],\n",
    "        target_modules = config['lora']['target_modules'],\n",
    "        lora_alpha = config['lora']['lora_alpha'],\n",
    "        lora_dropout = config['lora']['lora_dropout'], # Supports any, but = 0 is optimized\n",
    "        bias = config['lora']['bias'],    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = config['lora']['use_gradient_checkpointing'], # True or \"unsloth\" for very long context\n",
    "        random_state = config['lora']['random_state'],\n",
    "        max_seq_length = config['model']['max_seq_length'],\n",
    "    )\n",
    "\n",
    "    # dpo_trainer = DPOTrainer(\n",
    "    #     model = model,\n",
    "    #     ref_model = None,\n",
    "    #     args = DPOConfig(\n",
    "    #         per_device_train_batch_size = config['training']['per_device_train_batch_size'],\n",
    "    #         gradient_accumulation_steps = config['training']['gradient_accumulation_steps'],\n",
    "    #         warmup_ratio = config['training']['warmup_ratio'],\n",
    "    #         num_train_epochs = config['training']['num_train_epochs'],\n",
    "    #         fp16 = not is_bfloat16_supported(),\n",
    "    #         bf16 = is_bfloat16_supported(),\n",
    "    #         logging_steps = config['training']['logging_steps'],\n",
    "    #         optim = config['training']['optim'],\n",
    "    #         seed = config['training']['seed'],\n",
    "    #         output_dir = config['training']['output_dir'],\n",
    "    #     ),\n",
    "    #     beta = config['dpo']['beta'],\n",
    "    #     train_dataset = raw_datasets['train'],\n",
    "    #     # eval_dataset = YOUR_DATASET_HERE,\n",
    "    #     tokenizer = tokenizer,\n",
    "    #     max_length = config['dpo']['max_length'],\n",
    "    #     max_prompt_length = config['dpo']['max_prompt_length'],\n",
    "    # )\n",
    "\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model = model,\n",
    "        ref_model = None,\n",
    "        args = DPOConfig(\n",
    "            per_device_train_batch_size = 2,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            warmup_ratio = 0.1,\n",
    "            num_train_epochs = 3,\n",
    "            learning_rate = 5e-6,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.0,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 42,\n",
    "            output_dir = \"outputs\",\n",
    "            report_to = \"none\", # Use this for WandB etc\n",
    "        ),\n",
    "        beta = 0.1,\n",
    "        train_dataset = raw_datasets[\"train\"],\n",
    "        # eval_dataset = raw_datasets[\"test\"],\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 1024,\n",
    "        max_prompt_length = 512,\n",
    "    )\n",
    "\n",
    "    dpo_trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description=\"Load model with RLHF config:\")\n",
    "    # parser.add_argument(\"--config\", type=str, default=\"config.yaml\", help=\"Path to config file\")\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    logger.info(\"Start the dpo training process...\")\n",
    "    dpo_pipeline('/teamspace/studios/this_studio/LLM-Safety-Evaluation/configs/sailor_dpo_config.yaml')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
